---
name: ivr-test-execution
description: Execute and validate Amazon Connect IVR tests using Native Test API, Chat API, or manual approaches. Trigger on test IVR, run tests, execute tests, validate flow, or verify IVR.
---

# IVR Test Execution Skill

This skill **runs tests** against deployed Amazon Connect IVR flows. It covers three approaches — Native Test API (automated), Chat API (semi-automated), and manual testing via the Connect Console.

For **generating test script documents** (scenario descriptions), see the `ivr-test-scripts` skill. This skill takes those scripts and executes them.

## Quick Reference

| Approach | Guide | Best For |
|----------|-------|----------|
| **Python Test Harness** (RECOMMENDED) | Read [python-test-harness.md](python-test-harness.md) | Reproducible Native Test API execution |
| Native Test API (CLI commands) | Read [native-test-api.md](native-test-api.md) | One-off CLI testing |
| Chat API (semi-automated) | Read [chat-api-testing.md](chat-api-testing.md) | Lex bot + Lambda flows, QIC integration |
| Manual testing (console/phone) | Read [manual-testing.md](manual-testing.md) | Final validation, complex flows |

## Choosing an Approach

| Flow Characteristics | Recommended Approach |
|---------------------|---------------------|
| DTMF-only menus (`GetParticipantInput`) | Native Test API |
| Lex bot + simple responses | Native Test API (DTMF works, but timeouts possible) |
| Lex bot + Lambda + queue transfers | Chat API or Manual |
| Q in Connect (QIC) integration | Chat API or Manual |
| Complex branching, hours checks | Manual |
| Pre-production final validation | Manual (always) |

## Prerequisite Questions (MANDATORY)

**Before starting ANY work, you MUST ask the user these questions and wait for answers:**

### Question 1: Test Approach
Ask: **"How would you like to test this IVR?"**
- **Native Test API** — Automated testing via AWS CLI (`CreateTestCase` / `StartTestCaseExecution`)
- **Chat API** — Semi-automated via `StartChatContact` / `SendMessage` / `GetTranscript`
- **Manual** — Guide me through testing via Connect Console or phone call
- **Let me recommend** — I'll pick based on the flow's complexity

### Question 2: Flow Identification
Ask: **"Which flow should I test?"**
- **Flow ID** — User provides the contact flow ID directly
- **Flow name** — User provides the flow name; resolve via `aws connect list-contact-flows`
- **Test scripts file** — User provides a test scripts markdown file (generated by `ivr-test-scripts` skill)

### Question 3: AWS Access
You MUST ask:
- **AWS CLI profile name** (e.g., `haohai`, `default`, `prod`)
- **Amazon Connect Instance ID** — If not known, list instances: `aws connect list-instances --profile <PROFILE>`

### Confirmation Gate
After collecting answers, summarize:
> "I will test [flow name/ID] using [approach] on AWS profile `X`, instance `Y`. Is that correct?"

**Do NOT proceed until the user confirms.**

---

## Core Principles

1. **Generate Python test harness for reproducibility** — For Native Test API flows, ALWAYS generate a `run-tests.py` script. Do NOT just run CLI commands manually. The Python script ensures tests are reproducible and can be re-run later.
2. **Test against deployed flows** — Only test flows that are already deployed to the Connect instance. Never test draft JSON.
3. **Use test scripts as the source of truth** — Each test execution maps to a scenario from the test scripts document. Reference scenario IDs (S1, S2, etc.).
4. **Report results in the comparison format** — See "Comparison File Format" below. This is mandatory.
5. **Document limitations** — If a scenario can't be tested with the chosen approach, note it and suggest an alternative.
6. **Don't modify the flow** — Testing is read-only. Never change the flow to make a test pass.

## Cross-References

- **Flow design rules that affect testability:** See `amazon-connect-ivr` skill, "AWS Native Test API Compatibility Rules" (Rules 1-17). These rules describe how flow design choices impact what's observable in tests.
- **Lex bot testing compatibility:** See `amazon-lex-bot` skill, "Native Test API Compatibility" section. DTMF through Lex bots works correctly in Native Test API.
- **Test script generation:** See `ivr-test-scripts` skill to generate scenario documents before running tests.

## Python Test Harness Workflow (RECOMMENDED)

For **Native Test API** flows, the recommended approach is:

1. **Generate Python test script** from test-scripts.md using [python-test-harness.md](python-test-harness.md)
2. **Run the script:** `python3 run-tests.py`
3. **Collect results** from `test-execution-results.json`
4. **Generate results-and-comparison.md** using the Expected vs Actual format

This approach ensures tests are **reproducible**, **version-controllable**, and **CI/CD ready**.

## Alternative Workflows

### Native Test API (CLI commands)
1. Read [native-test-api.md](native-test-api.md)
2. Execute individual CLI commands for each scenario
3. Collect results manually

### Chat API (QIC flows)
1. Read [chat-api-testing.md](chat-api-testing.md)
2. Use `StartChatContact`, `SendMessage`, `GetTranscript` sequence
3. Capture WebSocket messages for real-time verification

### Manual Testing
1. Read [manual-testing.md](manual-testing.md)
2. Test via Connect Console or phone call
3. Document observations manually

## Output

Regardless of approach, always produce a single `results-and-comparison.md` file using the format below. This is the **only output file** — do NOT create separate `test-results.md` or `comparison.md` files.

---

## Results and Comparison File Format (MANDATORY)

After testing, produce a single `results-and-comparison.md` file. The format uses **Expected** and **Actual** sections for clarity. **Mismatches are marked with `>>> MISMATCH`.** Do NOT produce any other output files (no `test-results.md`, no `comparison.md`).

No summary tables, no methodology sections, no boilerplate. Just the scenario comparisons.

### Template (`results-and-comparison.md`)

```markdown
# [Flow Name] — Results and Comparison

**Flow:** [flow-id] | **Method:** [test method] | **Date:** [date]

---

## S1: [Scenario Name]

**Expected:**
1. System plays greeting: "Welcome to..."
2. System plays menu prompt
3. Customer presses 1
4. System plays confirmation message
5. Call disconnects

**Actual:**
1. ✅ System played greeting correctly
2. ✅ Menu prompt played correctly
3. ✅ DTMF "1" accepted
4. ✅ Confirmation message played
5. ✅ Call disconnected

**Result:** PASSED

---

## S2: [Scenario Name]

**Expected:**
1. System plays greeting
2. Customer presses 9 (invalid)
3. System plays error message: "Sorry, invalid selection..."
4. Call disconnects

**Actual:**
1. ✅ Greeting played correctly
2. ✅ DTMF "9" processed
3. ❌ **>>> MISMATCH** — System played: "We didn't catch that. Try again."
4. ✅ Call disconnected

**Result:** FAILED
```

### Rules

1. **Every scenario gets Expected and Actual sections.** No exceptions — even if it passed.
2. **Expected section** describes what should happen, extracted from test-scripts.md.
3. **Actual section** shows what really happened, with ✅ for matches and ❌ **>>> MISMATCH** for discrepancies.
4. **Use keywords "Expected" and "Actual"** as section headers for clarity.
5. **Result line** at the end: PASSED, FAILED, TIMEOUT, or SKIPPED.
6. **Skipped scenarios** get: "NOT TESTED" in Actual section.
7. **No summary section** required — the Result line suffices.
